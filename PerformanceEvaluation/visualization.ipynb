{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Tian Yun\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "target_domain = \"block\"  # [\"block\", \"kitchen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbcfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 14}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567010e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_category_tuples = {\n",
    "    \"Single Goal, Correct Steps\": [1, 2, 3],\n",
    "    \"Multiple Goal, Correct Steps\": [5, 6],  # skip case 4\n",
    "    \"Single Goal, Wrong Steps\": [7, 8, 9, 10],\n",
    "    \"Multiple Goal, Wrong Steps\": [11, 12],\n",
    "}\n",
    "\n",
    "baseline_agent_dict = {\n",
    "    \"random\": \"Random - Correct\",\n",
    "    \"htn\": \"HTN\",\n",
    "    \"fixed_always_ask\": \"ALWAYS-ASK\",\n",
    "    \"standard\": \"DGR-POMDP\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db90e796",
   "metadata": {},
   "source": [
    "# 1. Parsing HTN-based Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case_name(case_id):\n",
    "    # This function can be replaced by a case_id to case_category mapping\n",
    "    for case_cat, cases in case_category_tuples.items():\n",
    "        if case_id in cases:\n",
    "            return case_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcfadc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.base_dir_path = f\"./outputs/{target_domain}\"\n",
    "        self.agent_type = [\"htn\", \"fixed_always_ask\"]\n",
    "        self.hyperparams = \"dp17_sn5_df0.95_e1_wp-5_qr5_qp-5_oh0.76_dt0.001\"\n",
    "        self.reliability_scores = [0.8, 0.9, 0.95, 0.99]\n",
    "        self.num_goals = 5\n",
    "        self.num_cases = 2\n",
    "        self.category_list = [\"single_correct\", \"single_wrong\", \"multi_correct\", \"multi_wrong\"]        \n",
    "        \n",
    "args = parseArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load acc and std csv files\n",
    "goal_acc_csv_dict, goal_sem_csv_dict = {},{}\n",
    "plan_acc_csv_dict, plan_sem_csv_dict = {},{}\n",
    "\n",
    "for agent in args.agent_type:\n",
    "    # Load goal accuracy and standard error of mean\n",
    "    goal_acc_csv_dict[agent] = pd.read_csv(os.path.join(args.base_dir_path, agent, args.hyperparams, \"goal_accuracy.csv\"))\n",
    "    goal_sem_csv_dict[agent] = pd.read_csv(os.path.join(args.base_dir_path, agent, args.hyperparams, \"goal_std.csv\"))\n",
    "    \n",
    "    # Load plan accuracy and standard error of mean\n",
    "    plan_acc_csv_dict[agent] = pd.read_csv(os.path.join(args.base_dir_path, agent, args.hyperparams, \"plan_accuracy.csv\"))\n",
    "    plan_sem_csv_dict[agent] = pd.read_csv(os.path.join(args.base_dir_path, agent, args.hyperparams, \"plan_std.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213bd14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./outputs/block/htn/dp17_sn5_df0.95_e1_wp-5_qr5_qp-5_oh0.76_dt0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load runtime/num_questions/reward csv files\n",
    "cumu_runtime_mean_csv_dict, cumu_runtime_sem_csv_dict = {},{}\n",
    "cumu_questions_mean_csv_dict, cumu_questions_sem_csv_dict = {},{}\n",
    "cumu_reward_mean_csv_dict, cumu_reward_sem_csv_dict = {},{}\n",
    "\n",
    "for agent in args.agent_type:\n",
    "\n",
    "    df_mean_time = goal_acc_csv_dict[agent].copy()  # same dataframe format\n",
    "    df_sem_time = goal_acc_csv_dict[agent].copy()\n",
    "    df_mean_questions = goal_acc_csv_dict[agent].copy()  # same dataframe format\n",
    "    df_sem_questions = goal_acc_csv_dict[agent].copy()\n",
    "    df_mean_reward = goal_acc_csv_dict[agent].copy()  # same dataframe format\n",
    "    df_sem_reward = goal_acc_csv_dict[agent].copy()\n",
    "\n",
    "    for sensor_reliability in args.reliability_scores:\n",
    "        \n",
    "        row_id = 0\n",
    "        for case_id in range(1, 12+1):\n",
    "            \n",
    "            if case_id == 4 or case_id == 11 or case_id ==10 or case_id ==9 or case_id ==7:\n",
    "                # Case 4 is intended to be skipped\n",
    "                continue\n",
    "            row_id += 1\n",
    "\n",
    "            case_csv_name = f\"Episode-Case{case_id}_{sensor_reliability}.csv\"\n",
    "            curr_df = pd.read_csv(os.path.join(args.base_dir_path, agent, args.hyperparams, \"episode_reward\", case_csv_name))\n",
    "            x = []\n",
    "\n",
    "            # Process runtime\n",
    "            curr_mean_time = curr_df[\"normalized_time\"].mean()\n",
    "            curr_sem_time = curr_df[\"normalized_time\"].std() / np.sqrt(len(curr_df))\n",
    "            df_mean_time.loc[row_id, str(sensor_reliability)] = curr_mean_time\n",
    "            df_sem_time.loc[row_id, str(sensor_reliability)] = curr_sem_time\n",
    "            \n",
    "            # Process number of questions asked\n",
    "            curr_mean_questions = curr_df[\"normalized_num_question_asked\"].mean()\n",
    "            curr_sem_questions = curr_df[\"normalized_num_question_asked\"].std() / np.sqrt(len(curr_df))\n",
    "            df_mean_questions.loc[row_id, str(sensor_reliability)] = curr_mean_questions\n",
    "            df_sem_questions.loc[row_id, str(sensor_reliability)] = curr_sem_questions\n",
    "            \n",
    "            # Process discounted rewards\n",
    "            curr_mean_reward = curr_df[\"cumu_discounted_reward\"].mean()\n",
    "            curr_sem_reward = curr_df[\"cumu_discounted_reward\"].std() / np.sqrt(len(curr_df))\n",
    "            df_mean_reward.loc[row_id, str(sensor_reliability)] = curr_mean_reward\n",
    "            df_sem_reward.loc[row_id, str(sensor_reliability)] = curr_sem_reward\n",
    "\n",
    "    cumu_runtime_mean_csv_dict[agent] = df_mean_time.copy()\n",
    "    cumu_runtime_sem_csv_dict[agent] = df_sem_time.copy()\n",
    "    cumu_questions_mean_csv_dict[agent] = df_mean_questions.copy()\n",
    "    cumu_questions_sem_csv_dict[agent] = df_sem_questions.copy()\n",
    "    cumu_reward_mean_csv_dict[agent] = df_mean_reward.copy()\n",
    "    cumu_reward_sem_csv_dict[agent] = df_sem_reward.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b939670",
   "metadata": {},
   "source": [
    "# 2. Parsing SIPS Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6849bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./outputs/block/SIPS/optimal/block-words-0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb85be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.base_dir_path = f\"./outputs/{target_domain}\"\n",
    "        self.agent_type = [\"SIPS\"]\n",
    "        self.hyperparams = \"dp17_sn5_df0.95_e1_wp-5_qr5_qp-5_oh0.76_dt0.001\"\n",
    "        self.reliability_scores = [0.8, 0.9, 0.95, 0.99]\n",
    "        self.num_goals = 7\n",
    "        self.num_cases = 2\n",
    "        self.category_list = [\"single_correct\", \"single_wrong\", \"multi_correct\", \"multi_wrong\"]        \n",
    "        \n",
    "args = parseArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_acc_csv_dict, goal_sem_csv_dict = {},{}\n",
    "plan_acc_csv_dict, plan_sem_csv_dict = {},{}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing \"optimal\" folder, including [\"single_correct\", \"multi_correct\"]\n",
    "\n",
    "for agent in args.agent_type:\n",
    "    data_dir_path = os.path.join(args.base_dir_path, agent, \"optimal\")\n",
    "    \n",
    "    for sensor_reliability in args.reliability_scores:\n",
    "        \n",
    "        # \"single_correct\"\n",
    "        for goal in range(0, 6):\n",
    "            curr_df = pd.read_csv(\n",
    "                os.path.join(data_dir_path, f\"block-words-{sensor_reliability}\", f\"block-words_problem_0_goal{goal}.csv\")\n",
    "            )\n",
    "            break\n",
    "        \n",
    "        # \"multi_correct\"\n",
    "        for goal in range(6, 7):\n",
    "            curr_df = pd.read_csv(\n",
    "                os.path.join(data_dir_path, f\"block-words-{sensor_reliability}\", f\"block-words_problem_0_goal{goal}.csv\")\n",
    "            )\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1302ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0069a0",
   "metadata": {},
   "source": [
    "# 3. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8eabfc",
   "metadata": {},
   "source": [
    "## 3.1. Goal Accuracy vs. Reliability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f86a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get aggregated results for each case for each agent\n",
    "goal_acc_line_df = defaultdict(dict)\n",
    "goal_sem_line_df = defaultdict(dict)\n",
    "x_ticklabels = None\n",
    "\n",
    "for case, case_nums in case_category_tuples.items():\n",
    "    #print(case, case_nums)\n",
    "    for agent in args.agent_type:\n",
    "        curr_df = goal_acc_csv_dict[agent].copy()\n",
    "        #print(curr_df)\n",
    "        curr_df = curr_df[curr_df[\"Case_Num\"].isin(case_nums)]\n",
    "        curr_df.drop(\"Case_Num\", axis=1, inplace=True)\n",
    "        goal_acc_line_df[case][agent] = list(curr_df.mean().to_dict().values())[::-1]  # Make the sensor reliability ascending\n",
    "        \n",
    "        curr_df = goal_sem_csv_dict[agent].copy()\n",
    "        curr_df = curr_df[curr_df[\"Case_Num\"].isin(case_nums)]\n",
    "        curr_df.drop(\"Case_Num\", axis=1, inplace=True)\n",
    "        goal_sem_line_df[case][agent] = list(curr_df.mean().to_dict().values())[::-1] # Make the sensor reliability ascending\n",
    "        x_ticklabels = curr_df.columns.to_list()[::-1]  # Make the sensor reliability ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Performance by Case Category\n",
    "x = [float(val) for val in x_ticklabels]\n",
    "\n",
    "n_row = 2\n",
    "n_col = 2\n",
    "fig, ax = plt.subplots(n_row, n_col, figsize=(16,10))\n",
    "\n",
    "for case_id, case in enumerate(case_category_tuples.keys()):\n",
    "    \n",
    "    plot_row = case_id // n_col\n",
    "    plot_col = case_id % n_col\n",
    "\n",
    "    for agent in (args.agent_type):\n",
    "        acc = goal_acc_line_df[case][agent]\n",
    "        sem = goal_sem_line_df[case][agent]\n",
    "        ax[plot_row][plot_col].errorbar(x, acc, yerr=sem, label=baseline_agent_dict[agent])\n",
    "\n",
    "    ax[plot_row][plot_col].legend(loc=\"lower right\")\n",
    "    ax[plot_row][plot_col].set_title(case)\n",
    "    ax[plot_row][plot_col].set_ylabel('Goal Accuracy')\n",
    "    if plot_row == 1:\n",
    "        ax[plot_row][plot_col].set_xlabel('Sensor Reliability')\n",
    "        ax[plot_row][plot_col].set_ylim([0.5, 1.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ee0a3",
   "metadata": {},
   "source": [
    "## 3.2. Plan Accuracy vs. Reliability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f565920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get aggregated results for each case for each agent\n",
    "plan_acc_line_df = defaultdict(dict)\n",
    "plan_sem_line_df = defaultdict(dict)\n",
    "x_ticklabels = None\n",
    "\n",
    "for case, case_nums in case_category_tuples.items():\n",
    "    #print(case, case_nums)\n",
    "    for agent in args.agent_type:\n",
    "        curr_df = plan_acc_csv_dict[agent].copy()\n",
    "        #print(curr_df)\n",
    "        curr_df = curr_df[curr_df[\"Case_Num\"].isin(case_nums)]\n",
    "        curr_df.drop(\"Case_Num\", axis=1, inplace=True)\n",
    "        plan_acc_line_df[case][agent] = list(curr_df.mean().to_dict().values())[::-1]  # Make the sensor reliability ascending\n",
    "        \n",
    "        curr_df = plan_sem_csv_dict[agent].copy()\n",
    "        curr_df = curr_df[curr_df[\"Case_Num\"].isin(case_nums)]\n",
    "        curr_df.drop(\"Case_Num\", axis=1, inplace=True)\n",
    "        plan_sem_line_df[case][agent] = list(curr_df.mean().to_dict().values())[::-1] # Make the sensor reliability ascending\n",
    "        x_ticklabels = curr_df.columns.to_list()[::-1]  # Make the sensor reliability ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c4473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Performance by Case Category\n",
    "x = [float(val) for val in x_ticklabels]\n",
    "\n",
    "n_row = 2\n",
    "n_col = 2\n",
    "fig, ax = plt.subplots(n_row, n_col, figsize=(16,10))\n",
    "\n",
    "for case_id, case in enumerate(case_category_tuples.keys()):\n",
    "    \n",
    "    plot_row = case_id // n_col\n",
    "    plot_col = case_id % n_col\n",
    "\n",
    "    for agent in (args.agent_type):\n",
    "        acc = plan_acc_line_df[case][agent]\n",
    "        sem = plan_sem_line_df[case][agent]\n",
    "        ax[plot_row][plot_col].errorbar(x, acc, yerr=sem, label=baseline_agent_dict[agent])\n",
    "\n",
    "    ax[plot_row][plot_col].legend(loc=\"lower right\")\n",
    "    ax[plot_row][plot_col].set_title(case)\n",
    "    ax[plot_row][plot_col].set_ylabel('Goal Accuracy')\n",
    "    if plot_row == 1:\n",
    "        ax[plot_row][plot_col].set_xlabel('Sensor Reliability')\n",
    "        ax[plot_row][plot_col].set_ylim([0.5, 1.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85712082",
   "metadata": {},
   "source": [
    "## 3.3. Aggregating #Questions/Rewards/Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881ff81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6751d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b7535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a379d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4171e93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls outputs/block/SIPS/optimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef87b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.results_dir_path = \"./outputs/block\"\n",
    "        self.models = [\"htn\"]\n",
    "        self.category_list = [\"single_correct\", \"single_wrong\", \"multi_correct\", \"multi_wrong\"]\n",
    "        self.hyperparams = \"dp17_sn5_df0.95_e1_wp-5_qr5_qp-5_oh0.76_dt0.001\"\n",
    "        self.reliability_list = [0.8, 0.9, 0.95, 0.99]\n",
    "        self.num_goals = 5\n",
    "        self.num_cases = 2\n",
    "\n",
    "args = parseArguments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff86ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top1_accuracy(data_df, label, num_goals):\n",
    "    \"\"\" Top-1 accuracy for goal inference. \"\"\"\n",
    "    goal_inference_correct = 0\n",
    "    for row_id in range(len(data_df)):\n",
    "        row = data_df.iloc[row_id]\n",
    "        \n",
    "        all_goal_probs = []\n",
    "        for goal_id in range(args.num_goals):\n",
    "            all_goal_probs.append(row[f\"goal_probs_{goal_id}\"])\n",
    "        max_goal = max(all_goal_probs)\n",
    "        predicted_goals = np.where(all_goal_probs == max_goal)[0]\n",
    "        \n",
    "        if label in predicted_goals:\n",
    "            goal_inference_correct += 1\n",
    "            \n",
    "    accuracy = goal_inference_correct / len(data_df)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c91fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dict = {\n",
    "    model: {\n",
    "        \"single_correct\": defaultdict(dict), \n",
    "        \"single_wrong\": defaultdict(dict),\n",
    "        \"multi_correct\": defaultdict(dict),\n",
    "        \"multi_wrong\": defaultdict(dict),\n",
    "    }\n",
    "    for model in args.models\n",
    "}\n",
    "\n",
    "goal_acc_dict = template_dict.copy()\n",
    "goal_sem_dict = template_dict.copy()\n",
    "plan_acc_dict = template_dict.copy()\n",
    "plan_sem_dict = template_dict.copy()\n",
    "runtime_mean_dict = template_dict.copy()\n",
    "runtime_sem_dict = template_dict.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b56698a",
   "metadata": {},
   "source": [
    "# 1. Parse SIPS Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac1bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./outputs/block/SIPS/optimal/block-words-0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir_root = os.path.join(args.results_dir_path, \"SIPS\")\n",
    "\n",
    "for group in [\"optimal\", \"suboptimal\"]:\n",
    "    for reliability in [0.8, 0.9, 0.95, 0.99]:\n",
    "        data_dir_path = os.path.join(result_dir_root, group, f\"block-words-{reliability}\")\n",
    "        for goal_label in [0,1,2,3,4,5]\n",
    "            data = pd.read_csv(data_path)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d3e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6852e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e13e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./outputs/block/SIPS/optimal/block-words-0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc8a47",
   "metadata": {},
   "source": [
    "# 2. Parse Other Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d04826",
   "metadata": {},
   "source": [
    "# 3. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./outputs/block/htn/dp17_sn5_df0.95_e1_wp-5_qr5_qp-5_oh0.76_dt0.001/episode_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_category_case_map = {\n",
    "    \"single_correct\": [1,2,3,4,5],\n",
    "    \"single_wrong\": [6,7,8,9,10],\n",
    "    \"multi_correct\": [11,12,13,23,24,25,26,27,28,29],\n",
    "    \"multi_wrong\": [14,15,16,17,18,19,22,30],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9333652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in args.models:\n",
    "    data_dir_path = os.path.join(args.results_dir_path, model, args.hyperparams)\n",
    "    goal_acc_data = pd.read_csv(data_dir_path, \"goal_accuracy.csv\")\n",
    "    goal_std_data = pd.read_csv(data_dir_path, \"goal_std.csv\")\n",
    "    plan_acc_data = pd.read_csv(data_dir_path, \"plan_accuracy.csv\")\n",
    "    plan_std_data = pd.read_csv(data_dir_path, \"plan_std.csv\")\n",
    "    \n",
    "    data_dir_path = \n",
    "    for reliability in args.reliability_list:\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec98b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_acc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & process data\n",
    "\n",
    "for model in args.models:\n",
    "    \n",
    "    for reliability in args.reliability_list:\n",
    "        data_dir_name = f\"{output_name}-{reliability}\"\n",
    "\n",
    "        # Compute the average top-1 accuracy for Case 0 (correct steps)\n",
    "        acc_list, runtime_list = [], []\n",
    "        for goal_id in range(args.num_goals):\n",
    "            data_path = os.path.join(\n",
    "                args.results_dir_path, \n",
    "                data_dir_name, \n",
    "                f\"block-words_problem_0_goal{goal_id}.csv\"\n",
    "            )\n",
    "            data = pd.read_csv(data_path)\n",
    "            acc = get_top1_accuracy(data, goal_id, args.num_goals)\n",
    "            acc_list.append(acc)\n",
    "            runtime_list.append(data[\"step_durs\"].mean())\n",
    "            \n",
    "        acc_list = np.array(acc_list)\n",
    "        #print(acc_list)\n",
    "        runtime_list = np.array(runtime_list)\n",
    "        acc_dict[model][\"correct_steps\"][str(reliability)] = acc_list.mean()\n",
    "        sem_dict[model][\"correct_steps\"][str(reliability)] = acc_list.std() / np.sqrt(len(acc_list))\n",
    "        runtime_mean_dict[model][\"correct_steps\"][str(reliability)] = runtime_list.mean()\n",
    "        runtime_sem_dict[model][\"correct_steps\"][str(reliability)] = runtime_list.std() / np.sqrt(len(runtime_list))\n",
    "\n",
    "        # Compute the average top-1 accuracy for Case 1 & 2 (wrong steps)\n",
    "        acc_list, runtime_list = [], []\n",
    "        for case_id in range(args.num_cases):\n",
    "            for goal_id in range(args.num_goals):\n",
    "                data_path = os.path.join(\n",
    "                    args.results_dir_path, \n",
    "                    data_dir_name, \n",
    "                    f\"block-words_problem_0_goal{goal_id}_{case_id}.csv\"\n",
    "                )\n",
    "                data = pd.read_csv(data_path)\n",
    "                acc = get_top1_accuracy(data, goal_id, args.num_goals)\n",
    "                acc_list.append(acc)\n",
    "                runtime_list.append(data[\"step_durs\"].mean())\n",
    "        acc_list = np.array(acc_list)\n",
    "        print(acc_list)\n",
    "        runtime_list = np.array(runtime_list)\n",
    "        acc_dict[model][\"wrong_steps\"][str(reliability)] = acc_list.mean()\n",
    "        sem_dict[model][\"wrong_steps\"][str(reliability)] = acc_list.std() / np.sqrt(len(acc_list))\n",
    "        runtime_mean_dict[model][\"wrong_steps\"][str(reliability)] = runtime_list.mean()\n",
    "        runtime_sem_dict[model][\"wrong_steps\"][str(reliability)] = runtime_list.std() / np.sqrt(len(runtime_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3f3e2",
   "metadata": {},
   "source": [
    "# 1. Top-1 Accuracy vs. Sensor Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde299f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row = 1\n",
    "n_col = 2\n",
    "fig, ax = plt.subplots(n_row, n_col, figsize=(12,4))\n",
    "\n",
    "for plot_id, category in enumerate(args.category_list):\n",
    "    x = acc_dict[model][category].keys()\n",
    "    acc_list = acc_dict[model][category].values()\n",
    "    sem_list = sem_dict[model][category].values()\n",
    "\n",
    "    ax[plot_id].errorbar(x, acc_list, yerr=sem_list, label=model)\n",
    "\n",
    "    ax[plot_id].legend()\n",
    "    ax[plot_id].set_title(category, fontweight=\"bold\")\n",
    "    ax[plot_id].set_ylabel(\"Top1 Accuracy\", fontweight=\"bold\")\n",
    "    ax[plot_id].set_xlabel(\"Sensor Reliability\", fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4140caa9",
   "metadata": {},
   "source": [
    "# 2. Runtime vs. Sensor Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row = 1\n",
    "n_col = 2\n",
    "fig, ax = plt.subplots(n_row, n_col, figsize=(12,4))\n",
    "\n",
    "for plot_id, category in enumerate(args.category_list):\n",
    "    x = runtime_mean_dict[model][category].keys()\n",
    "    runtime_mean = runtime_mean_dict[model][category].values()\n",
    "    runtime_sem = runtime_sem_dict[model][category].values()\n",
    "\n",
    "    ax[plot_id].errorbar(x, runtime_mean, yerr=runtime_sem, label=model)\n",
    "\n",
    "    ax[plot_id].legend()\n",
    "    ax[plot_id].set_title(category, fontweight=\"bold\")\n",
    "    ax[plot_id].set_ylabel(\"Runtime\", fontweight=\"bold\")\n",
    "    ax[plot_id].set_xlabel(\"Sensor Reliability\", fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad802c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77731bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coachdial",
   "language": "python",
   "name": "coachdial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
